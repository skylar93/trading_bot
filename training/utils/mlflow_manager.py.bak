"""MLflow experiment tracking manager"""

import os
import mlflow
from mlflow.entities import RunStatus
import logging
import warnings
import yaml
from typing import Dict, Any, Optional, List, Tuple
import pandas as pd
from pathlib import Path
from datetime import datetime
import sys
import torch
import shutil
import time
import tempfile
import torch.nn as nn

logger = logging.getLogger(__name__)

class MLflowManager:
    """Manages MLflow experiment tracking with standardized paths and formats."""
    
    REQUIRED_COLUMNS = {"$open", "$high", "$low", "$close", "$volume"}
    
    def __init__(
        self,
        experiment_name: str = "default",
        tracking_dir: str = "./mlflow_runs",
        max_retries: int = 3,
        retry_delay: float = 1.0
    ):
        """
        Initialize MLflow experiment manager.
        
        Args:
            experiment_name: Name of the MLflow experiment
            tracking_dir: Base directory for MLflow runs
            max_retries: Maximum number of retries for MLflow operations
            retry_delay: Delay between retries in seconds
        """
        self.experiment_name = experiment_name
        self.tracking_dir = Path(tracking_dir).absolute()
        self._active_run = None
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        # Create tracking directory if it doesn't exist
        os.makedirs(self.tracking_dir, exist_ok=True)
        
        # Set MLflow tracking URI to local directory
        tracking_uri = f"file://{self.tracking_dir}"
        mlflow.set_tracking_uri(tracking_uri)
        logger.debug(f"Set MLflow tracking URI to: {tracking_uri}")
        
        # Create or get experiment
        for attempt in range(self.max_retries):
            try:
                # Check if experiment exists
                experiment = mlflow.get_experiment_by_name(experiment_name)
                if experiment:
                    # Delete existing experiment
                    mlflow.delete_experiment(experiment.experiment_id)
                    time.sleep(self.retry_delay)
                    
                    # Verify deletion
                    deleted_exp = mlflow.get_experiment(experiment.experiment_id)
                    if deleted_exp.lifecycle_stage != "deleted":
                        continue
                
                # Create new experiment
                logger.debug(f"Creating new experiment: {experiment_name}")
                artifact_location = str(self.tracking_dir / experiment_name)
                os.makedirs(artifact_location, exist_ok=True)
                
                self.experiment_id = mlflow.create_experiment(
                    experiment_name,
                    artifact_location=artifact_location
                )
                
                # Set the active experiment
                mlflow.set_experiment(experiment_name)
                
                # Verify experiment creation
                time.sleep(self.retry_delay)
                created_exp = mlflow.get_experiment(self.experiment_id)
                if created_exp and created_exp.name == experiment_name:
                    break
                    
            except Exception as e:
                if attempt == self.max_retries - 1:
                    logger.error(f"Failed to initialize MLflow experiment after {self.max_retries} attempts: {str(e)}")
                    raise
                time.sleep(self.retry_delay)
                continue

    def start_run(
        self,
        run_name: Optional[str] = None,
        nested: bool = False,
        tags: Optional[Dict[str, str]] = None
    ) -> mlflow.ActiveRun:
        """Start a new MLflow run with optional name and tags."""
        if self._active_run and not nested:
            self.end_run()
            
        if nested and not self._active_run:
            raise mlflow.exceptions.MlflowException("No active parent run found for nested run")
        
        # Start the run with retries
        for attempt in range(self.max_retries):
            try:
                self._active_run = mlflow.start_run(
                    experiment_id=self.experiment_id,
                    run_name=run_name,
                    nested=nested,
                    tags=tags
                )
                
                # Verify run creation
                time.sleep(self.retry_delay)
                run_id = self._active_run.info.run_id
                run = mlflow.get_run(run_id)
                
                if run and run.info.run_id == run_id:
                    # Log a dummy metric to ensure meta.yaml is created
                    mlflow.log_metric("_dummy", 0.0)
                    time.sleep(self.retry_delay)
                    
                    # Verify meta.yaml exists
                    meta_path = Path(run.info.artifact_uri.replace("file://", "")) / "meta.yaml"
                    if meta_path.exists():
                        break
                        
            except Exception as e:
                if attempt == self.max_retries - 1:
                    logger.error(f"Failed to start MLflow run after {self.max_retries} attempts: {str(e)}")
                    raise
                time.sleep(self.retry_delay)
                continue
                
        return self._active_run

    def end_run(self, status: str = RunStatus.FINISHED) -> None:
        """End current MLflow run."""
        if self._active_run:
            try:
                run_id = self._active_run.info.run_id
                
                # End run with retries
                for attempt in range(self.max_retries):
                    try:
                        # Log final dummy metric
                        mlflow.log_metric("_dummy_end", 0.0)
                        time.sleep(self.retry_delay)
                        
                        # End the run
                        mlflow.end_run(status=status)
                        time.sleep(self.retry_delay)
                        
                        # Verify run ended
                        run = mlflow.get_run(run_id)
                        if run.info.status == status:
                            break
                            
                    except Exception as e:
                        if attempt == self.max_retries - 1:
                            logger.error(f"Failed to end MLflow run after {self.max_retries} attempts: {str(e)}")
                            raise
                        time.sleep(self.retry_delay)
                        continue
                        
            finally:
                self._active_run = None

    def log_model(self, model: nn.Module, artifact_path: str) -> None:
        """Log a PyTorch model.
        
        Args:
            model: PyTorch model to log
            artifact_path: Path within the run's artifact directory
        """
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run to log model to")
            
        # Save model to temporary file
        temp_dir = tempfile.mkdtemp()
        try:
            model_path = os.path.join(temp_dir, "model.pth")
            torch.save(model.state_dict(), model_path)
            
            # Log a dummy metric to ensure meta.yaml exists
            mlflow.log_metric("_dummy_model", 0.0)
            time.sleep(0.5)
            
            # Log the model file as an artifact
            mlflow.log_artifact(model_path, artifact_path)
            
            # Wait for logging to complete
            time.sleep(0.5)
            
        finally:
            shutil.rmtree(temp_dir)

    def log_artifact(
        self,
        local_path: str,
        artifact_path: Optional[str] = None
    ) -> None:
        """Log a local file as an MLflow artifact."""
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        # Log a dummy metric to ensure meta.yaml exists
        mlflow.log_metric("_dummy_artifact", 0.0)
        time.sleep(0.5)
        
        # Log artifact to MLflow
        mlflow.log_artifact(local_path, artifact_path)
        
        # Wait for logging to complete
        time.sleep(0.5)

    @property
    def experiment(self) -> str:
        """Get the experiment ID."""
        return self.experiment_id

    @property
    def active_run(self) -> Optional[mlflow.ActiveRun]:
        """Get the current active run."""
        return self._active_run

    def __enter__(self):
        """Context manager entry"""
        self.start_run()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.end_run()
        return False

    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:
        """Log metrics to current run."""
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        mlflow.log_metrics(metrics, step=step)
        
        # Wait for logging to complete
        time.sleep(0.5)

    def log_params(self, params: Dict[str, Any]) -> None:
        """Log parameters to current run."""
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        mlflow.log_params(params)
        
        # Wait for logging to complete
        time.sleep(0.5)

    def log_dataframe(
        self,
        df: pd.DataFrame,
        artifact_path: str,
        artifact_name: Optional[str] = None
    ) -> None:
        """Log DataFrame as an artifact in parquet format."""
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        # Validate DataFrame columns
        missing_columns = self.REQUIRED_COLUMNS - set(df.columns)
        if missing_columns:
            warnings.warn(
                f"DataFrame missing required columns with $ prefix: {missing_columns}",
                UserWarning
            )
        
        if artifact_name is None:
            artifact_name = "data.parquet"
        
        if not artifact_name.endswith(".parquet"):
            artifact_name += ".parquet"
            
        # Create temporary directory
        temp_dir = tempfile.mkdtemp()
        try:
            # Save DataFrame to temporary file
            temp_file = os.path.join(temp_dir, artifact_name)
            df.to_parquet(temp_file)
            
            # Log the file as an artifact
            mlflow.log_artifact(temp_file, artifact_path)
            
            # Wait for logging to complete
            time.sleep(0.5)
            
        finally:
            shutil.rmtree(temp_dir)

    def log_backtest_results(
        self,
        results: Dict[str, Any],
        artifact_path: str = "backtest"
    ) -> None:
        """Log backtest results to MLflow.
        
        Args:
            results: Dictionary containing backtest results
            artifact_path: Path to store artifacts
        """
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        # Log metrics (handle nested dictionaries)
        metrics = {}
        for key, value in results.items():
            if isinstance(value, dict):
                # Handle nested metrics
                for nested_key, nested_value in value.items():
                    if isinstance(nested_value, (int, float)) and not isinstance(nested_value, bool):
                        metrics[nested_key] = nested_value
            elif isinstance(value, (int, float)) and not isinstance(value, bool):
                metrics[key] = value
                
        if metrics:
            self.log_metrics(metrics)
            
        # Log DataFrames
        for key, value in results.items():
            if isinstance(value, pd.DataFrame):
                self.log_dataframe(
                    value,
                    f"{artifact_path}/data",
                    f"{key}.parquet"
                )

    def cleanup(self) -> None:
        """Clean up MLflow resources."""
        if self._active_run:
            self.end_run()
            
        # Delete experiment if it exists
        try:
            experiment = mlflow.get_experiment_by_name(self.experiment_name)
            if experiment:
                # End any active runs
                for run in mlflow.search_runs([experiment.experiment_id]):
                    if run.info.status == "RUNNING":
                        mlflow.end_run(run_id=run.info.run_id)
                        time.sleep(0.1)
                
                # Delete experiment
                mlflow.delete_experiment(experiment.experiment_id)
                time.sleep(0.5)  # Wait for deletion to complete
                
                # Clean up experiment directory
                experiment_dir = os.path.join(self.tracking_dir, self.experiment_name)
                if os.path.exists(experiment_dir):
                    try:
                        shutil.rmtree(experiment_dir)
                    except:
                        pass
        except Exception as e:
            logger.warning(f"Failed to delete experiment {self.experiment_id}: {str(e)}")
            
        # Clean up MLflow directories
        mlflow_dirs = [
            "./mlruns",
            "./mlflow_runs",
            os.path.join(os.getcwd(), "mlruns"),
            os.path.join(os.getcwd(), "mlflow_runs")
        ]
        for d in mlflow_dirs:
            if os.path.exists(d):
                try:
                    shutil.rmtree(d)
                except:
                    pass

    def get_artifact_uri(self, artifact_path: str) -> str:
        """Get the URI for an artifact.
        
        Args:
            artifact_path: Path to the artifact
            
        Returns:
            The URI for the artifact
        """
        if not self._active_run:
            raise mlflow.exceptions.MlflowException("No active run exists")
            
        return mlflow.get_artifact_uri(artifact_path)
