"""Tests for MLflow experiment tracking"""

import pytest
import pandas as pd
import numpy as np
import mlflow
from pathlib import Path
import shutil
import tempfile
import os
from training.utils.mlflow_manager import MLflowManager
from unittest.mock import patch, MagicMock
import time

@pytest.fixture(autouse=True)
def mlflow_test_setup():
    """Setup MLflow for testing with a temporary tracking URI and test experiment."""
    # Create temporary directory for MLflow tracking
    with tempfile.TemporaryDirectory() as tmp_dir:
        tracking_uri = f"file://{tmp_dir}"
        mlflow.set_tracking_uri(tracking_uri)
        
        # Create test experiment
        try:
            experiment_id = mlflow.create_experiment(
                "test_experiment",
                artifact_location=os.path.join(tmp_dir, "test_experiment")
            )
        except mlflow.exceptions.MlflowException:
            # If experiment exists, delete it and recreate
            experiment = mlflow.get_experiment_by_name("test_experiment")
            if experiment:
                mlflow.delete_experiment(experiment.experiment_id)
                time.sleep(1)  # Wait for deletion
                experiment_id = mlflow.create_experiment(
                    "test_experiment",
                    artifact_location=os.path.join(tmp_dir, "test_experiment")
                )
        
        mlflow.set_experiment("test_experiment")
        
        yield
        
        # Cleanup
        if mlflow.active_run():
            mlflow.end_run()
            
        # No need to cleanup directory as tempfile will handle it

@pytest.fixture
def mlflow_manager():
    """Create MLflowManager instance for testing."""
    return MLflowManager()

def test_mlflow_manager_initialization(mlflow_manager):
    """Test basic MLflowManager initialization."""
    assert mlflow_manager is not None
    assert mlflow.get_experiment_by_name("test_experiment") is not None

def test_start_run(mlflow_manager):
    """Test starting a new MLflow run."""
    with mlflow_manager.start_run() as run:
        assert mlflow.active_run() is not None
        assert run.info.experiment_id == mlflow.get_experiment_by_name("test_experiment").experiment_id

def test_log_metrics(mlflow_manager):
    """Test logging metrics to MLflow."""
    metrics = {
        "reward": 100.0,
        "sharpe_ratio": 1.5
    }
    
    with mlflow_manager.start_run():
        mlflow_manager.log_metrics(metrics)
        current_run = mlflow.active_run()
        run_data = mlflow.get_run(current_run.info.run_id).data
        
        assert run_data.metrics["reward"] == 100.0
        assert run_data.metrics["sharpe_ratio"] == 1.5

def test_log_params(mlflow_manager):
    """Test logging parameters to MLflow."""
    params = {
        "learning_rate": 0.001,
        "batch_size": 64
    }
    
    with mlflow_manager.start_run():
        mlflow_manager.log_params(params)
        current_run = mlflow.active_run()
        run_data = mlflow.get_run(current_run.info.run_id).data
        
        assert run_data.params["learning_rate"] == "0.001"
        assert run_data.params["batch_size"] == "64"

def test_nested_runs(mlflow_manager):
    """Test handling of nested runs."""
    with mlflow_manager.start_run() as parent_run:
        # Log something in parent run
        mlflow_manager.log_metrics({"parent_metric": 1.0})
        
        with mlflow_manager.start_run(nested=True) as child_run:
            # Log something in child run
            mlflow_manager.log_metrics({"child_metric": 2.0})
            
            assert mlflow.active_run().info.run_id == child_run.info.run_id
            
        # After child run ends, we should be back to parent run
        assert mlflow.active_run().info.run_id == parent_run.info.run_id

@pytest.mark.skip("Implement if artifact logging is needed")
def test_log_artifact():
    """Test logging artifacts to MLflow."""
    pass

def test_end_run(mlflow_manager):
    """Test ending MLflow runs properly."""
    mlflow_manager.start_run()
    assert mlflow.active_run() is not None
    
    mlflow_manager.end_run()
    assert mlflow.active_run() is None

@pytest.fixture
def sample_data():
    """Create sample OHLCV data with proper column names"""
    return pd.DataFrame({
        "$open": [100, 101, 102],
        "$high": [102, 103, 104],
        "$low": [99, 100, 101],
        "$close": [101, 102, 103],
        "$volume": [1000, 1100, 1200]
    })

@pytest.fixture
def mlflow_test_context():
    """Create a temporary directory for MLflow testing"""
    temp_dir = tempfile.mkdtemp()
    experiment_name = "test_experiment"
    
    # Set up MLflow environment
    os.environ["MLFLOW_TRACKING_URI"] = f"file://{temp_dir}"
    
    yield {
        "temp_dir": temp_dir,
        "experiment_name": experiment_name
    }
    
    # Cleanup after test
    if mlflow.active_run():
        mlflow.end_run()
    
    try:
        shutil.rmtree(temp_dir)
    except Exception as e:
        print(f"Warning: Failed to cleanup temp directory: {e}")

@pytest.fixture
def mlflow_manager(mlflow_test_context):
    """Create MLflowManager instance with test context"""
    manager = MLflowManager(
        experiment_name=mlflow_test_context['experiment_name'],
        tracking_dir=mlflow_test_context['temp_dir']
    )
    
    yield manager
    
    # End any active runs
    if mlflow.active_run():
        mlflow.end_run()

def test_mlflow_manager_initialization(mlflow_manager, mlflow_test_context):
    """Test MLflowManager initialization"""
    assert mlflow_manager.experiment_name == mlflow_test_context['experiment_name']
    assert mlflow_manager.tracking_dir == Path(mlflow_test_context['temp_dir']).absolute()

def test_run_lifecycle(mlflow_manager, sample_data):
    """Test complete MLflow run lifecycle"""
    with mlflow_manager.start_run(run_name="test_run"):
        # Log parameters
        test_params = {
            "learning_rate": 0.001,
            "batch_size": 64,
            "window_size": 20
        }
        mlflow_manager.log_params(test_params)
        
        # Log metrics
        test_metrics = {
            "train_loss": 0.5,
            "val_loss": 0.6,
            "sharpe_ratio": 1.2
        }
        mlflow_manager.log_metrics(test_metrics, step=0)
        
        # Log DataFrame
        mlflow_manager.log_dataframe(sample_data, "data", "test_data.parquet")
        
        # Get current run ID before ending
        run_id = mlflow.active_run().info.run_id
    
    # Verify run data after completion
    run = mlflow.get_run(run_id)
    assert run.data.params["batch_size"] == "64"
    assert run.data.metrics["train_loss"] == 0.5
    assert run.data.metrics["sharpe_ratio"] == 1.2

def test_artifact_format(mlflow_manager, sample_data):
    """Test artifact saving and loading with proper column names"""
    with mlflow_manager.start_run():
        mlflow_manager.log_dataframe(sample_data, "data", "test.parquet")
        
        # Get artifact path
        artifact_uri = mlflow_manager.get_artifact_uri("data/test.parquet")
        artifact_path = Path(artifact_uri.replace("file://", ""))
        
        # Verify artifact exists and can be loaded
        assert artifact_path.exists()
        loaded_df = pd.read_parquet(artifact_path)
        pd.testing.assert_frame_equal(sample_data, loaded_df)
        
        # Verify required columns
        assert all(col in loaded_df.columns for col in MLflowManager.REQUIRED_COLUMNS)

def test_error_handling(mlflow_manager):
    """Test error handling scenarios"""
    # Test logging without active run
    with pytest.raises(mlflow.exceptions.MlflowException):
        mlflow_manager.log_metrics({"test": 1.0})
    
    # Test invalid DataFrame
    invalid_df = pd.DataFrame({"wrong_column": [1, 2, 3]})
    with mlflow_manager.start_run():
        with pytest.warns(UserWarning, match="DataFrame missing required columns"):
            mlflow_manager.log_dataframe(invalid_df, "data", "invalid.parquet")

def test_backtest_results(mlflow_manager, sample_data):
    """Test logging backtest results"""
    backtest_results = {
        "sharpe_ratio": 1.5,
        "max_drawdown": 0.2,
        "total_return": 0.4,
        "trades": pd.DataFrame({
            "timestamp": pd.date_range("2024-01-01", periods=3),
            "type": ["buy", "sell", "buy"],
            "price": [100, 110, 105],
            "size": [1.0, 1.0, 0.5]
        })
    }
    
    with mlflow_manager.start_run():
        mlflow_manager.log_backtest_results(backtest_results)
        
        # Verify metrics were logged
        run = mlflow.get_run(mlflow.active_run().info.run_id)
        assert run.data.metrics["sharpe_ratio"] == 1.5
        assert run.data.metrics["max_drawdown"] == 0.2
        assert run.data.metrics["total_return"] == 0.4
        
        # Verify trades DataFrame was saved
        artifact_uri = mlflow_manager.get_artifact_uri("backtest/data/trades.parquet")
        trades_df = pd.read_parquet(artifact_uri.replace("file://", ""))
        pd.testing.assert_frame_equal(trades_df, backtest_results["trades"])

def test_best_run_selection(mlflow_manager):
    """Test getting best run based on metric"""
    metric_name = "sharpe_ratio"
    
    # Create multiple runs with different metrics
    for i, value in enumerate([1.0, 2.0, 1.5]):
        with mlflow_manager.start_run(run_name=f"run_{i}"):
            mlflow_manager.log_metrics({metric_name: value})
    
    # Get best run (highest value)
    best_run_id, best_value = mlflow_manager.get_best_run(metric_name)
    assert best_value == 2.0
    
    # Get best run (lowest value)
    best_run_id, best_value = mlflow_manager.get_best_run(metric_name, ascending=True)
    assert best_value == 1.0

if __name__ == "__main__":
    pytest.main(["-v", __file__])